{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e738ac85",
   "metadata": {},
   "source": [
    "# Welcome to the DCOM 2023 How Models learn Hands On Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09439fba",
   "metadata": {},
   "source": [
    "The business problem is to help the fictitious ***Boston DCom Real State agency*** to be able to provide reliable price estimation for houses in the Boston area, analysing the factors that influence the pricing of properties. \n",
    "\n",
    "For that, we will use a dataset that has been widely used for educational purposes: it is not intended to reflect any current state of the real state market. The the data columns (features) are the following: \n",
    "\n",
    "    1. CRIM      per capita crime rate by town\n",
    "    2. ZN        proportion of residential land zoned for lots over \n",
    "                 25,000 sq.ft.\n",
    "    3. INDUS     proportion of non-retail business acres per town\n",
    "    4. CHAS      Charles River dummy variable (= 1 if tract bounds \n",
    "                 river; 0 otherwise)\n",
    "    5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "    6. RM        average number of rooms per dwelling\n",
    "    7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "    8. DIS       weighted distances to five Boston employment centres\n",
    "    9. RAD       index of accessibility to radial highways\n",
    "    10. TAX      full-value property-tax rate per 10,000 dollars \n",
    "    11. PTRATIO  pupil-teacher ratio by town\n",
    "    12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    13. LSTAT    % lower status of the population\n",
    "    14. MEDV     Median value of owner-occupied homes in 1k dollars's\n",
    "\n",
    "In this dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (MEDV). \n",
    "\n",
    "Therefore the problem consists in: given the set of features above describing a Boston neighbourhood, your machine learning model must predict the median house price in that area. Since the model will predict a continous value (not a discrete, ordinal value or a category),  this a **regression** task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb0509",
   "metadata": {},
   "source": [
    "Let's download the dataset from the UCI machine learning repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/' 'machine-learning-databases' '/housing/housing.data', sep='\\s+', names = names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b6309",
   "metadata": {},
   "source": [
    "# Plotting Some important values\n",
    "\n",
    "The pairplot function in the Seaborn library is a useful data visualization tool that helps to plot pairwise relationships between multiple variables in a dataset. It creates a grid of scatterplots and histograms showing the relationship between each pair of variables in the dataset.\n",
    "\n",
    "For example, if you have a dataset with multiple columns, the pairplot function can help you to quickly visualize the relationship between each column. The scatterplots in the diagonal of the grid show the distribution of each variable, while the scatterplots in the off-diagonal show the relationship between two variables.\n",
    "\n",
    "This function can be helpful in identifying patterns and correlations between variables, and can be used to gain insights into the relationships between different features of your data. Overall, the pairplot function is a useful tool for exploratory data analysis and can help to quickly identify potential areas of interest for further investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2163a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cols = ['DIS', 'RAD', 'NOX']\n",
    "\n",
    "sns.pairplot(df[cols], height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7cc200",
   "metadata": {},
   "source": [
    "We can observe, for instance, that the closer to the Boston working centres, the highest the concentration of Nox particles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727af071",
   "metadata": {},
   "source": [
    "# Exercise \n",
    "\n",
    "Can you tell how crime per capita by town is related to the houses median values rate using pair plots as above? What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b27c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['CRIM', 'MEDV']\n",
    "\n",
    "sns.pairplot(df[cols], height=2.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0d5fc",
   "metadata": {},
   "source": [
    "# How features are related?\n",
    "\n",
    "\n",
    "If you have two sets of data and you want to know if they are related in some way, the numpy corrcoef function can help you to quantify that relationship. It returns a correlation matrix that shows the correlation coefficients between each pair of variables.\n",
    "\n",
    "The correlation coefficient is a value between -1 and 1 that indicates the strength and direction of the relationship between two variables. A value of 1 indicates a perfect positive correlation, which means that as one variable increases, the other variable also increases. A value of -1 indicates a perfect negative correlation, which means that as one variable increases, the other variable decreases. A value of 0 indicates no correlation between the variables.\n",
    "\n",
    "The numpy corrcoef function can be helpful in many different contexts, including scientific research, finance, and machine learning. It is a useful tool for understanding the relationship between different variables and can help to identify potential areas of interest for further investigation.\n",
    "\n",
    "The Seaborn Heatmap function is a data visualization tool that can help you understand the relationship between two variables. It works by taking in a two-dimensional array, where each element represents a value for a specific pair of variables.\n",
    "\n",
    "The function then creates a color-coded grid that displays the value of each element in the array. The color of each square in the grid is determined by the value of the corresponding element in the array, with higher values typically represented by brighter colors.\n",
    "\n",
    "The Seaborn Heatmap function is particularly useful for identifying patterns and correlations between variables, as it allows you to quickly see which pairs of variables have a strong relationship. It can be used to explore complex datasets and can help to identify potential areas of interest for further analysis.\n",
    "\n",
    "Overall, the Seaborn Heatmap function is a powerful tool for visualizing complex data and can be used to gain insights into the relationships between different variables in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']\n",
    "\n",
    "cm = np.corrcoef(df[cols].values.T)\n",
    "\n",
    "hm = sns.heatmap(cm,\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.2f',\n",
    "                 annot_kws={'size': 15},\n",
    "                 yticklabels=cols,\n",
    "                 xticklabels=cols)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7d148",
   "metadata": {},
   "source": [
    "By observing the last row of the plot, we remark that all features except RM, the average number of rooms, are negatively correlated with the median house price for a given neighbourhood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342e9de",
   "metadata": {},
   "source": [
    "# Training the model: reducing the loss iteractively\n",
    "\n",
    "The code block below represents an implementation of a linear regression model using stochastic gradient descent (SGD) algorithm. The class LinearRegressionGD has three methods: __init__, fit, net_input, and predict.\n",
    "\n",
    "The __init__ method initializes the learning rate eta and the number of iterations n_iter. The default values are eta=0.001 and n_iter=20.\n",
    "\n",
    "The fit method trains the model by updating the weights based on the error between the predicted output and the actual output. The weights w_ are initialized as zeros plus an additional element for the bias term. The method iterates over the training data X for n_iter times. In each iteration, it computes the predicted output using self.net_input(X) method, calculates the error between predicted and actual output, and updates the weights using the gradient descent rule. The bias term is updated separately. Finally, the method stores the cost or mean squared error in the cost_ list for each iteration.\n",
    "\n",
    "The net_input method computes the dot product between the input features X and the weight coefficients w_. It returns the predicted output.\n",
    "\n",
    "The predict method is a helper method that uses the net_input method to return the predicted output.\n",
    "\n",
    "Overall, the LinearRegressionGD class implements a simple linear regression model using SGD algorithm for updating the weights in a batch mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22467760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD(object):\n",
    "\n",
    "    def __init__(self, eta=0.001, n_iter=20):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w_ = np.zeros(1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.net_input(X)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.net_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156eca5b",
   "metadata": {},
   "source": [
    "For an elementary model we use a single feature, RM, the average number of rooms, in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['RM']].values\n",
    "y = df['MEDV'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9d606c",
   "metadata": {},
   "source": [
    "Scaling features is an important step in machine learning because it can help improve the performance and stability of many algorithms. Feature scaling ensures that all features have the same range, which can prevent certain features from dominating others, especially in algorithms that are sensitive to feature magnitudes. Furthermore, feature scaling can help the optimization algorithm converge faster and produce better results.\n",
    "\n",
    "StandardScaler in scikit-learn is a popular method to scale features. It scales the features such that they have a mean of 0 and a variance of 1. This method subtracts the mean from each feature and divides by the standard deviation, which is calculated using the formula (x - mean) / std. This transformation ensures that each feature has a similar scale, which is essential for many machine learning algorithms, especially those that rely on distances or gradients, such as K-nearest neighbors or support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_std = sc_x.fit_transform(X)\n",
    "y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d776cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegressionGD()\n",
    "lr.fit(X_std, y_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6fe3f6-c98d-4914-8b1a-ccb14d889ff2",
   "metadata": {},
   "source": [
    "Here we plot the model's loss, that is, the sum of squared error acroos the training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc29383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, lr.n_iter+1), lr.cost_)\n",
    "plt.ylabel('SSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673869d-28fa-402c-92db-13063eeb30bd",
   "metadata": {},
   "source": [
    "The function below plots the model's predictions in black while the actual values are plotted in steelblue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b22329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regplot(X, y, model):\n",
    "    plt.scatter(X, y, c='steelblue', edgecolor='white', s=70)\n",
    "    plt.plot(X, model.predict(X), color='black', lw=2)    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4a720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regplot(X_std, y_std, lr)\n",
    "plt.xlabel('Average number of rooms [RM] (standardized)')\n",
    "plt.ylabel('Price in $1000s [MEDV] (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a42ff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Slope: %.3f' % lr.w_[1])\n",
    "print('Intercept: %.3f' % lr.w_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d90261b",
   "metadata": {},
   "source": [
    "## Predicting the house prices\n",
    "For a neighbourhood where the average number of rooms is 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65439019",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rooms_std = sc_x.transform(np.array([[4.0]]))\n",
    "price_std = lr.predict(num_rooms_std)\n",
    "print(\"Price in $1000s: %.3f\" % sc_y.inverse_transform(price_std.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c65e5",
   "metadata": {},
   "source": [
    "# Visualizing model performance\n",
    "\n",
    "In the code block, the 'residual' is the difference between the predicted values (y_pred) and the actual target values (y_std), which are the values on the y-axis.\n",
    "\n",
    "The plot shows a scatter plot of the residuals against the predicted values on the x-axis. The residuals are calculated as the difference between the predicted values and the actual target values.\n",
    "\n",
    "The plot is useful for analyzing the performance of a regression model because it helps to identify any patterns or trends in the differences between the predicted and actual values. If the plot shows a random distribution of points around the horizontal line at y=0, then the model is likely making good predictions. However, if there is a pattern in the plot (such as a curve or a slope), it suggests that the model may be making systematic errors or biases in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6113129d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_std,  y_pred - y_std,\n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "            label='Training data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f293e",
   "metadata": {},
   "source": [
    "# Exercsise: \n",
    "What is the median price of a 2-bedroom appartment according to this model? Is it reasonalble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616957ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a50f28ab",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "The Rˆ2 score (also called the coefficient of determination) is an important measure of how well a regression model fits the data. It is a number between 0 and 1 that represents the proportion of the variance in the target variable (the variable being predicted) that is explained by the model.\n",
    "\n",
    "In simpler terms, the Rˆ2 score tells us how much of the variation in the target variable can be explained by the independent variables (the variables used to make predictions). A high Rˆ2 score (close to 1) means that the model is able to explain a large proportion of the variation in the target variable, while a low Rˆ2 score (close to 0) means that the model is not able to explain much of the variation.\n",
    "\n",
    "The Rˆ2 score is important because it helps us to evaluate the performance of a regression model. A high Rˆ2 score indicates that the model is making accurate predictions, while a low Rˆ2 score suggests that the model needs to be improved or that there may be problems with the data. Additionally, the Rˆ2 score can be used to compare the performance of different models and to choose the best one for a particular task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b712519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE train: %.3f' % (\n",
    "        mean_squared_error(y_std, y_pred),\n",
    "        ))\n",
    "print('R^2 train: %.3f' % (\n",
    "        r2_score(y_std, y_pred),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8cc8c",
   "metadata": {},
   "source": [
    "# Building a more robust model\n",
    "\n",
    "First, create a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1527b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['MEDV'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc8c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = LinearRegression()\n",
    "\n",
    "slr.fit(X_train, y_train)\n",
    "y_train_pred = slr.predict(X_train)\n",
    "y_test_pred = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e09a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Slope: %.3f' % slr.coef_[0])\n",
    "print('Intercept: %.3f' % slr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fb641a",
   "metadata": {},
   "source": [
    "We can see a way more random distrubution of points, showing that our linear model now preforms way better than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775017b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_train_pred,  y_train_pred - y_train,\n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "            label='Training data')\n",
    "plt.scatter(y_test_pred,  y_test_pred - y_test,\n",
    "            c='limegreen', marker='s', edgecolor='white',\n",
    "            label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color='black', lw=2)\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1fbdf-e6e1-4839-a6f5-056dad86f621",
   "metadata": {},
   "source": [
    "We can now evaluate how our model performs on training and on (unseen) test data, by looking at the loss and at the R2 Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac16f9d",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Find in the Sklearn library more advanced regression methods and try to get the the Rˆ2 test score above 0.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508f426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da87991",
   "metadata": {},
   "source": [
    "# Submit your results \n",
    "\n",
    "Use the holdout test set here: https://github.wdf.sap.corp/gist/i061767/84a0873c6d210559dc7a51b31d20ef6b#file-holdout_test_set-csv \n",
    "\n",
    "Predict the house prices and submit your CSV file to the instructor. The participant who obtains the greatest R2 score on the holdout test data wins the prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137cac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('holdout_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa4e4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
