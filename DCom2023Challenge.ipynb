{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2fb717",
   "metadata": {},
   "source": [
    "# Welcome to the DCOM 2023 How Models Learn Hands On Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df8c5e",
   "metadata": {},
   "source": [
    "<img src=\"dcom23back.jpg\" width=\"800\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10652f87",
   "metadata": {},
   "source": [
    "So we've got this fake real estate agency called California DCom and they want our help figuring out how to price houses in the Boston area. To do that, we're gonna take a look at some data and figure out what factors influence how much a house costs.\n",
    "\n",
    "We're gonna be using this dataset called California Housing Data, which is pretty popular for teaching people about this kind of stuff. It's not meant to be super up-to-date or anything, just good for learning.\n",
    "\n",
    "The dataset has a bunch of columns with info like the median income of people in the area, the average number of rooms and bedrooms in a household, and the population size. Plus, there's the latitude and longitude of each area, which is kinda cool.\n",
    "\n",
    "The thing we're trying to predict here is the median value of a house in each area, given in hundreds of thousands of dollars. So it's a **regression problem**, meaning we're trying to predict a number instead of just putting things into categories.\n",
    "\n",
    "Oh, and fun fact - the dataset was made from info gathered in the 1990 US census, with one row of data per \"census block group\". Basically, that just means a group of people living in a certain area.\n",
    "\n",
    "And one last thing to keep in mind - some areas in the dataset might have crazy big values for things like number of rooms or bedrooms. That's because those columns are looking at the average per household, and some areas might have lots of empty houses or vacation homes.\n",
    "\n",
    "🎯 The **learning objectives** are: \n",
    "1. Gain an understanding of hyperparameters and their role in deep learning.\n",
    "2. Learn how to evaluate model performance and compare different models using metrics like mean squared error (MSE) and coefficient of determination (R-squared).\n",
    "\n",
    "Dive into the world of ML and gain valuable experience in hyperparameter tuning and deep learning, enter your results to the leaderboard, and  Good luck to all participants!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9475c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LayerNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import History\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128767d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "california_housing.frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35209c24",
   "metadata": {},
   "source": [
    "# Visualizing house price distributions\n",
    "\n",
    "So we've the longitude and latitude that tell us where the districts from the dataset are on a map. And we're thinking maybe we can use that info to figure out if certain spots have really expensive houses or not.\n",
    "\n",
    "To test that out, we made this scatter plot thingy where the horizontal (x) axis is latitude and the vertical (y) axis is longitude. And then we made these circles that show how big and colorful they are depending on how much the houses in that area are worth. It's a pretty cool way to see if there are any trends or patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64744fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "california_img=mpimg.imread('california.png')\n",
    "sns.scatterplot(data=california_housing.frame, x=\"Longitude\", y=\"Latitude\",\n",
    "                size=\"MedHouseVal\", hue=\"MedHouseVal\",\n",
    "                palette=\"viridis\", alpha=0.5)\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "plt.legend(title=\"MedHouseVal\", bbox_to_anchor=(1.05, 0.95),\n",
    "           loc=\"upper left\")\n",
    "_ = plt.title(\"Median house value depending of\\n their spatial location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1fc53",
   "metadata": {},
   "source": [
    "So if you're not really familiar with California, you might not know that all these data points we're looking at actually make a map of the state. And it's pretty cool because we can see that the houses that are worth the most are all huddled up along the coast where the big cities are, like San Diego, Los Angeles, San Jose, and San Francisco. Guess people really like living by the ocean and in the city, huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4eb958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# Maybe you will want to fix the random state variable to an integer of your choice\n",
    "random_state = [42]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b11afc",
   "metadata": {},
   "source": [
    "# Challenge: find the hyperparameter values that will provide the closest estimation for the real house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfdc369",
   "metadata": {},
   "source": [
    "## Deep neural networks\n",
    "\n",
    "So we're gonna be training this thing called a deep neural network - it's a type of machine learning that's actually inspired by the way our own brains work! The network is made up of tons of little nodes called neurons, which work together to analyze patterns and make predictions.\n",
    "\n",
    "Basically, each neuron takes input from other neurons and uses that input to make its own calculations. Then it spits out an output based on all that input. And when you put all those neuron outputs together, you get the final output of the network.\n",
    "\n",
    "Neural networks are super helpful when you're dealing with a ton of data and patterns that are just too complex for us humans to figure out on our own. They're used all the time in things like image recognition, natural language processing, and predicting stuff in fields like finance, medicine, and engineering.\n",
    "\n",
    "Now, it's worth noting that there are plenty of other algorithms out there for regression that might be easier to use and work just as well or even better. But in this challenge, we're gonna focus on this deep neural network thing and learn how to tune it up to make it work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78971c1e",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "Okay, so this next part is all about the way we set up our model to predict how much houses in California are worth. We made a function called create_model that creates this neural network thing that does the predicting.\n",
    "\n",
    "Basically, the neural network is made up of a bunch of layers that are all connected to each other, and we use something called ReLU to help make the connections stronger. We also use something called layer normalization and dropout to help the network learn faster and avoid making too many mistakes.\n",
    "\n",
    "We put all this together and use a loss function called mean squared error to make sure our predictions are accurate, and then we use this thing called Adam optimizer to help make the predictions even better. And at the end, we have a model that we can use to make predictions about house prices in California. Cool, right?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0175bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape = 8, learning_rate = 0.1 , num_hidden_layers = 1, \n",
    "                 num_neurons_per_layer = 32 , dropout_prob = 0.1):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons_per_layer, input_shape=input_shape, activation='relu'))\n",
    "    model.add(LayerNormalization(axis=1))\n",
    "    model.add(Dropout(dropout_prob, input_shape=(2,)))\n",
    "    \n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_neurons_per_layer, activation='relu'))\n",
    "        model.add(LayerNormalization(axis=1))\n",
    "        model.add(Dropout(dropout_prob, input_shape=(2,)))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5a8db",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Okay, so you know how when you're cooking something, you might adjust the heat, the amount of salt, or the cooking time to get it just right? Well, in machine learning, we have something kind of similar called hyperparameter tuning.\n",
    "\n",
    "Basically, when we're building a machine learning model, we have to make some choices about how it's going to work. These choices are called hyperparameters, and they include things like how many layers a neural network should have, how many nodes are in each layer, and how quickly the model should learn from the data.\n",
    "\n",
    "Hyperparameter tuning is the process of experimenting with different choices for these hyperparameters to try and find the best combination for a particular problem. It's kind of like adjusting the heat or seasoning when you're cooking - you try different settings until you get the best result.\n",
    "\n",
    "The goal of hyperparameter tuning is to create a machine learning model that is as accurate as possible, while also being efficient enough to work quickly and not use up too many resources. It can be a bit of trial and error, but it's an important part of building a good machine learning model.\n",
    "\n",
    "## Set the parameters by changing the variables below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66116c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have an idea of how long it took you to find your solution.\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b7b1b4",
   "metadata": {},
   "source": [
    "### Batch Size: \n",
    "The batch size is how many examples the model looks at together when it's learning. A bigger batch size can make things move more quickly, but it might also make the model less precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d076a44",
   "metadata": {},
   "source": [
    "### Epochs: \n",
    "So, an epoch is basically when the network goes through all the training data once. By adjusting the number of epochs, you can decide how many times the network should go through the training data. If you increase the number of epochs, the model can become more accurate, but it can also start to overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20081fc1",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "The learning rate is like the gas pedal for the neural network during training. If the learning rate is high, the network goes full speed ahead and updates the weights more aggressively, but it can make the model less accurate. On the other hand, if the learning rate is low, the network takes it slow and steady, making more conservative updates to the weights, which can make the model more accurate, but also slower to train. So it's like a trade-off between speed and accuracy, and you have to find the sweet spot that works best for your problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1dab2e",
   "metadata": {},
   "source": [
    "### Number of Hidden Layers: \n",
    "Hidden layers are kind of like secret layers of neurons inside a neural network that don't talk directly to the input or output layers. The more of these hidden layers there are, the more complex patterns the network can learn, but it can also make it harder to train and more likely to overfit. So, adding more hidden layers can make the network stronger, but also riskier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de224eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90197033",
   "metadata": {},
   "source": [
    "### Number of Neurons per Layer: \n",
    "The number of neurons in a hidden layer determines how much the network can learn and how complex it can be. The more neurons in a layer, the more powerful the network is, but it can also cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42569336",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_per_layer = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da4e65",
   "metadata": {},
   "source": [
    "### Dropout Probability:\n",
    "\n",
    "Dropout is like a bouncer that randomly kicks out a percentage of neurons in each layer during training. The dropout probability decides how many neurons are removed. If we increase the dropout probability, it can make the model less likely to overfit and more robust, but it might also lower its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a19668",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81853e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "input_shape = (X_train.shape[1],)\n",
    "model = create_model(input_shape, learning_rate, num_hidden_layers, num_neurons_per_layer,dropout_prob)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c9b82",
   "metadata": {},
   "source": [
    "## Launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[history], verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028db7d",
   "metadata": {},
   "source": [
    "# Is your model overfitting?\n",
    "\n",
    "Overfitting happens when a deep neural network learns the training data too well, but starts to perform poorly on new data. Here are some ways to check if your model is overfitting:\n",
    "\n",
    "Check the training and validation loss: During training, **keep an eye on the training and validation loss**. If the training loss keeps decreasing while the validation loss starts to increase or level off, it might be a sign of overfitting. This means that the model is memorizing the training data too well and is not doing a good job of generalizing to new data.\n",
    "\n",
    "Use regularization techniques: Regularization techniques, such as dropout, can help prevent overfitting by adding noise to the network during training. This can force the network to learn more robust features that generalize better to new data.\n",
    "\n",
    "Get more data: One of the best ways to prevent overfitting is to get more data. This can help the network learn more representative patterns and reduce the chance of memorizing specific examples.\n",
    "\n",
    "By using these methods, you can analyze and prevent overfitting in your deep neural network, resulting in a more accurate and robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccedd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c173a4",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "\n",
    "When evaluating a regression model, you can use the R2 score and Mean Squared Error (MSE) as metrics to determine its performance.\n",
    "\n",
    "The **R2 score** is a measure of how well the model fits the data, ranging from 0 to 1. A higher R2 score means a better fit, with a score of 1 indicating a perfect prediction and a score of 0 indicating the model performs no better than predicting the mean value of the target variable.\n",
    "\n",
    "Another commonly used metric is **Mean Squared Error (MSE)**, which measures the average squared difference between the predicted and actual values of the target variable. The lower the MSE, the better the model's performance.\n",
    "\n",
    "These metrics allow you to compare different regression models and choose the one that fits the data best. R2 score is a good metric because it captures the variability in the data, while MSE is good because it penalizes large errors more than small ones, and can be easily interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8bdb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "\n",
    "print('MSE on test data: %.3f' % (\n",
    "        mean_squared_error(y_test, preds)))\n",
    "print('R^2 score on test data: %.3f' % (\n",
    "        r2_score(y_test, preds)))\n",
    "print(\"Your execution time was %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd51648c",
   "metadata": {},
   "source": [
    "Hey, just so you know, there are other ways to check if your model is actually solving the problem it's supposed to solve. Even if the metrics we talked about earlier seem okay, it's a good idea to check the residual plot to see if the errors are distributed evenly across all possible house prices. And keep in mind that sometimes the dataset just doesn't have enough information to make accurate predictions about median house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec375d",
   "metadata": {},
   "source": [
    " # Submit your results 🏁 \n",
    "\n",
    "Enter the R2 score and the MSE loss above to the leaderboard:\n",
    "https://sap-my.sharepoint.com/:l:/p/anderson_santana_de_oliveira/FLIQ2iJMYj1CkQFq2hp-X7oBahYvlAAuO5IwcEqppqulSg?e=7BHEwK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89eff6",
   "metadata": {},
   "source": [
    "# Optional: Predict Prices around Palo Alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb988228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delimiting a zone around Palo Alto\n",
    "xmin, xmax = -122.5, -121.5\n",
    "ymin, ymax = 37.2, 38.2\n",
    "\n",
    "# Filtering the dataframe to show the prices around of Palo Alto\n",
    "df = california_housing.frame\n",
    "palo_alto_df = df.loc[(df['Longitude'] >= xmin) & (df['Longitude'] <= xmax) & (df['Latitude'] >= ymin) & (df['Latitude'] <= ymax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3.7k entries in this perimeter\n",
    "palo_alto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets select randomly a handful of entries to predict\n",
    "palo_alto_df = palo_alto_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember we have to scale the features before we predict the price\n",
    "palo_alto_input = scaler.fit_transform(palo_alto_df.drop('MedHouseVal', axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's save the predictions in the Palo Alto dataframe\n",
    "palo_alto_df['Predictions'] = model.predict(palo_alto_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3bca22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "california_img=mpimg.imread('california.png')\n",
    "sns.scatterplot(data=palo_alto_df, x=\"Longitude\", y=\"Latitude\",\n",
    "                size=\"MedHouseVal\", hue=\"MedHouseVal\",\n",
    "                palette=\"viridis\", alpha=0.5)\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "plt.legend(title=\"MedHouseVal\", bbox_to_anchor=(1.05, 0.95),\n",
    "           loc=\"upper left\")\n",
    "_ = plt.title(\"Real Median house value for 5 instances in Palo Alto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de53a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_img=mpimg.imread('california.png')\n",
    "sns.scatterplot(data=palo_alto_df, x=\"Longitude\", y=\"Latitude\",\n",
    "               hue=\"Predictions\",  size=\"Predictions\",  \n",
    "                palette=\"viridis\", alpha=0.5)\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n",
    "plt.legend(title=\"Predicted Prices\", bbox_to_anchor=(1.05, 0.95),\n",
    "           loc=\"upper left\")\n",
    "_ = plt.title(\"Predicted median house value for 5 instances in Palo Alto\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
